{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFKHzHo64Ybreiv/2UBCkc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohumedraslan/Handwritten-Digit-Recognition-with-TensorFlow/blob/main/MNIST_Handwritten_Character_Recognition_with_mindspore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smbUE_4GKmjJ"
      },
      "outputs": [],
      "source": [
        "# Import related dependent libraries.\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import mindspore as ms\n",
        "import mindspore.context as context\n",
        "import mindspore.dataset as ds\n",
        "import mindspore.dataset.transforms.c_transforms as C\n",
        "import mindspore.dataset.vision.c_transforms as CV\n",
        "from mindspore.nn.metrics import Accuracy\n",
        "from mindspore import nn\n",
        "from mindspore.train import Model\n",
        "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
        "context.set_context(mode=context.GRAPH_MODE, device_target='CPU')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR_TRAIN = \"MNIST/train\" # Training set information\n",
        "DATA_DIR_TEST = \"MNIST/test\" # Test set information\n",
        "# Read data.\n",
        "ds_train = ds.MnistDataset(DATA_DIR_TRAIN)\n",
        "ds_test = ds.MnistDataset(DATA_DIR_TEST )\n",
        "# Display the dataset features.\n",
        "print('Data volume of the training dataset:',ds_train.get_dataset_size())\n",
        "print('Data volume of the test dataset:',ds_test.get_dataset_size())\n",
        "image=ds_train.create_dict_iterator().__next__()\n",
        "print('Image length/width/channels:',image['image'].shape)\n",
        "print('Image label style:',image['label']) # Total 10 label classes which are represented by\n",
        "numbers from 0 to 9."
      ],
      "metadata": {
        "id": "0_NuUq_tKsg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(training=True, batch_size=128, resize=(28, 28),\n",
        "  rescale=1/255, shift=0, buffer_size=64):\n",
        "  ds = ms.dataset.MnistDataset(DATA_DIR_TRAIN if training else DATA_DIR_TEST)\n",
        "  # Define the resizing, normalization, and channel conversion of the map operation.\n",
        "  resize_op = CV.Resize(resize)\n",
        "  rescale_op = CV.Rescale(rescale,shift)\n",
        "  hwc2chw_op = CV.HWC2CHW()\n",
        "  # Perform the map operation on the dataset.\n",
        "  ds = ds.map(input_columns=\"image\", operations=[rescale_op,resize_op, hwc2chw_op])\n",
        "  ds = ds.map(input_columns=\"label\", operations=C.TypeCast(ms.int32))\n",
        "  # Set the shuffle parameter and batch size.\n",
        "  ds = ds.shuffle(buffer_size=buffer_size)\n",
        "  ds = ds.batch(batch_size, drop_remainder=True)\n",
        "return ds\n"
      ],
      "metadata": {
        "id": "_i0_wrsIKulf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 images and the labels, and check whether the images are correctly labeled.\n",
        "ds = create_dataset(training=False)\n",
        "data = ds.create_dict_iterator().__next__()\n",
        "images = data['image'].asnumpy()\n",
        "labels = data['label'].asnumpy()\n",
        "plt.figure(figsize=(15,5))\n",
        "for i in range(1,11):\n",
        "plt.subplot(2, 5, i)\n",
        "plt.imshow(np.squeeze(images[i]))\n",
        "plt.title('Number: %s' % labels[i])\n",
        "plt.xticks([])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6q2RKYMNKzOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model. The model consists of three fully connected layers. The final output layer uses\n",
        "softmax for classification (10 classes consisting of numbers 0 to 9.)\n",
        "class ForwardNN(nn.Cell):\n",
        "def __init__(self):\n",
        "super(ForwardNN, self).__init__()\n",
        "self.flatten = nn.Flatten()\n",
        "self.fc1 = nn.Dense(784, 512, activation='relu')\n",
        "self.fc2 = nn.Dense(512, 128, activation='relu')\n",
        "self.fc3 = nn.Dense(128, 10, activation=None)\n",
        "def construct(self, input_x):\n",
        "output = self.flatten(input_x)\n",
        "output = self.fc1(output)\n",
        "output = self.fc2(output)\n",
        "output = self.fc3(output)\n",
        "return output"
      ],
      "metadata": {
        "id": "-h_5G3BdK023"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a network, loss function, validation metric, and optimizer, and set related hyperparameters.\n",
        "lr = 0.001\n",
        "num_epoch = 10\n",
        "momentum = 0.9\n",
        "net = ForwardNN()\n",
        "loss = nn.loss.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
        "metrics={\"Accuracy\": Accuracy()}\n",
        "opt = nn.Adam(net.trainable_params(), lr)"
      ],
      "metadata": {
        "id": "atlJJVgfK2iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model.\n",
        "model = Model(net, loss, opt, metrics)\n",
        "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
        "ckpoint_cb = ModelCheckpoint(prefix=\"checkpoint_net\",directory = \"./ckpt\" ,config=config_ck)\n",
        "# Generate a dataset.\n",
        "ds_eval = create_dataset(False, batch_size=32)\n",
        "ds_train = create_dataset(batch_size=32)\n",
        "# Train the model.\n",
        "loss_cb = LossMonitor(per_print_times=1875)\n",
        "time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n",
        "print(\"============== Starting Training ==============\")\n",
        "model.train(num_epoch, ds_train,callbacks=[ckpoint_cb,loss_cb,time_cb ],dataset_sink_mode=False)"
      ],
      "metadata": {
        "id": "6el-k3taK5kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the test set to validate the model and print the overall accuracy.\n",
        "metrics=model.eval(ds_eval)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "5lsGj7-uK7Tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}